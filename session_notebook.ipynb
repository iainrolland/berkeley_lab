{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e5751b5",
   "metadata": {},
   "source": [
    "### Check dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ac0a56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "libraries = [\n",
    "    \"tensorflow\",\n",
    "    \"numpy\",\n",
    "    \"spektral\",\n",
    "    \"matplotlib\",\n",
    "    \"pandas\",\n",
    "    \"sklearn\"\n",
    "]\n",
    "\n",
    "for lib in libraries:\n",
    "    try:\n",
    "        if not importlib.util.find_spec(lib):\n",
    "            raise ImportError(\"Package '%s' is not installed\" % lib)\n",
    "    except ImportError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac47b9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc75bc8",
   "metadata": {},
   "source": [
    "### Soil moisture data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49441ad6",
   "metadata": {},
   "source": [
    "The SMAP data has already been cropped into numpy arrays for the three regions of interest:\n",
    "- Italy (`Italy_data.npy`)\n",
    "- California (`California_data.npy`)\n",
    "- South Africa (`SouthAfrica_data.npy`)\n",
    "\n",
    "and labels:\n",
    "\n",
    "- Italy (`labels/Italy.npy`)\n",
    "- California (`labels/California.npy`)\n",
    "- South Africa (`labels/SouthAfrica.npy`)\n",
    "\n",
    "Using `load_italy()` as example, write functions to do the same for California/South Africa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc02cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import completed\n",
    "\n",
    "def load_italy():\n",
    "    return np.load(\"Italy_data.npy\"), np.load(os.path.join(\"labels\", \"Italy.npy\"))\n",
    "\n",
    "def load_calif():\n",
    "    \"\"\"\n",
    "    TASK 1: Write function to read and return California data (can just copy load_italy() and change file name)\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "    return\n",
    "\n",
    "def load_southafrica():\n",
    "    \"\"\"\n",
    "    TASK 2: Write function to read and return SouthAfrica data (can just copy load_italy() and change file name)\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "    return\n",
    "\n",
    "def load_data(area):\n",
    "    regions = [\"italy\", \"california\", \"southafrica\"]\n",
    "    if area not in regions:\n",
    "        raise ValueError(\"'region' must be one of {} but was {}\".format(regions, region))\n",
    "    else:\n",
    "        if area == \"italy\":\n",
    "            return load_italy()\n",
    "        if area == \"california\":\n",
    "            return load_calif()\n",
    "        if area == \"southafrica\":\n",
    "            return load_southafrica()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea07bc5",
   "metadata": {},
   "source": [
    "Let's visualise this data, the following produces a plot of the data at 10 time stamps (each 5 days after the previous):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5ffb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "region = \"italy\"\n",
    "data, label_map = load_data(region)\n",
    "\n",
    "for i in range(10):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.matshow(data[..., i * 10])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f80661",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Max: %s\" % np.nanmax(data))\n",
    "print(\"Min: %s\" % np.nanmin(data))\n",
    "print(\"Missing value ratio: %s\" % (np.sum(np.isnan(data)) / np.prod(data.shape)))\n",
    "print(\"Data shape: {}\".format(data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d421a99d",
   "metadata": {},
   "source": [
    "**Linearly interpolates to fill missing data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8701cbba",
   "metadata": {},
   "source": [
    "To fill the missing values, we will use a simple linear interpolation approach.\n",
    "There is quick method for doing so which uses the `pandas` library to make a `Dataframe` that has linear interpolation functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046b2eee",
   "metadata": {},
   "source": [
    "This function (and most of the code) assumes we are working with a 2-dimensional array which has nodes along the first dimension and time along the second (and last) dimension. To do this, we will flatten the spatial dimensions of the SMAP data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f79aeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_interp(data_values):\n",
    "    \"\"\"\n",
    "    data: array (Last axis must be time)\n",
    "    \"\"\"\n",
    "    ndim = data_values.ndim\n",
    "    in_shape = data_values.shape\n",
    "    if ndim > 2:\n",
    "        data_values = data_values.reshape(-1, data_values.shape[-1])\n",
    "    return pd.DataFrame(data_values.T).interpolate(method=\"linear\", limit_direction=\"forward\").interpolate(method=\"linear\",\n",
    "                                                                                                    limit_direction=\"backward\").values.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82809905",
   "metadata": {},
   "source": [
    "To show this in operation, here is an example, before and after interpolation. Note that if there are no observed values over time the `nan` values aren't filled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912fd1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_array = np.array([[1., np.nan, np.nan, 7.], [np.nan, 2., 3., np.nan], [np.nan, np.nan, np.nan, np.nan], [np.nan, 3., np.nan, 5]])\n",
    "\n",
    "print(\"Before:\")\n",
    "print(test_array)\n",
    "print()\n",
    "print(\"After:\")\n",
    "print(lin_interp(test_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87c7505",
   "metadata": {},
   "source": [
    "**Write a function to normalize the data between 0 and 1 (if not already)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87d6a86",
   "metadata": {},
   "source": [
    "Generally speaking, it is common practice to normalize features in the data before using deep learning methods. This ensures that gradients will be comparable across features and applications which means that there is less need to tune optimization parameters such as learning rates/momentum etc.\n",
    "\n",
    "We will again assume that the function input will be a 2-dimensional array with #nodes x #timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe97613a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data_values):\n",
    "    \"\"\"\n",
    "    TASK 3: Write a function to normalize data to between 0 and 1\n",
    "    data_values: 2-d numpy array with nodes along first axis and time along second axis\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f032d80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_array = np.array(np.array([[1,2,3],[4,1,1], [3., 1., 3.5]]))\n",
    "\n",
    "print(\"Before:\")\n",
    "print(test_array)\n",
    "print()\n",
    "print(\"After:\")\n",
    "print(normalize(test_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d48685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(data_values):\n",
    "    \"\"\"\n",
    "    TASK 4: Write a function to take the 3-dimensional data (space x space x time) \n",
    "    and flatten into a 2-d array with time as second axis\n",
    "    \n",
    "    data_values: numpy array (ndim=3)\n",
    "    \n",
    "    return:\n",
    "    data_values: numpy array (ndim=2)\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38e07cf",
   "metadata": {},
   "source": [
    "**Use our pre-processing functions to prepare our data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd86706",
   "metadata": {},
   "outputs": [],
   "source": [
    "filled = lin_interp(flatten(data))\n",
    "normalized = normalize(filled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c750c87",
   "metadata": {},
   "source": [
    "Let's revisualize data now it has been interpolated/normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463ad595",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped = normalized.reshape(data.shape)\n",
    "\n",
    "for i in range(10):\n",
    "    fig, ax = plt.subplots()\n",
    "    cm = ax.matshow(normalized.reshape(data.shape)[..., i * 10])\n",
    "    fig.colorbar(cm, ax=ax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb57a8f9",
   "metadata": {},
   "source": [
    "Alternatively, we can take specific pixels and look at their time series across the course of the year after interpolation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f485d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    plt.plot(normalized[i * 10])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1cecb0",
   "metadata": {},
   "source": [
    "For the pixels which still have no data (e.g. pixels representing sea) we will throw them away when we reshape our data into a two-dimensional array. We will want to save this mask though in order to recover the spatial view when visualizing our final results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b2f2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_seen = np.all(np.isnan(normalized), axis=1)\n",
    "model_inputs = normalized[~not_seen]  # this gives array of shape (#nodes, #timesteps) without the unobserved regions\n",
    "\n",
    "plt.matshow(not_seen.reshape(data.shape[:2]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54824068",
   "metadata": {},
   "source": [
    "# Uncertainty-aware graph-based classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46871f53",
   "metadata": {},
   "source": [
    "The labels to be used as ground truth for this demonstration can be plotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c04365",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "cm = ax.matshow(label_map, cmap=plt.cm.Accent)\n",
    "fig.colorbar(cm, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7a6971",
   "metadata": {},
   "source": [
    "To flatten our labels and remove the pixels which are not observed, we can use our `not_seen` mask.\n",
    "\n",
    "This gives `labels` in shape (#nodes,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9979a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Labels shape: {}\".format(label_map.shape))\n",
    "\n",
    "labels = label_map.flatten()[~not_seen]\n",
    "print(\"Labels (masked) shape: {}\".format(labels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc61183",
   "metadata": {},
   "source": [
    "One-hot encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b04a57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "labels_oh = OneHotEncoder().fit_transform(labels.reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f47199",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"labels_oh.shape: {}\".format(labels_oh.shape))\n",
    "print(\"first 5 labels:{}\".format(labels[:5]))\n",
    "print(\"first 5 rows:\\n{}\".format(labels_oh[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a96bd43",
   "metadata": {},
   "source": [
    "The inputs and labels should now be of appropriate shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3add57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_inputs.shape)\n",
    "print(labels_oh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd58074c",
   "metadata": {},
   "source": [
    "#### Task 1: Write function to perform dataset split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e53579e",
   "metadata": {},
   "source": [
    "Write a function which will randomly assign each node into one of:\n",
    "\n",
    "- training\n",
    "- validation\n",
    "- testing\n",
    "\n",
    "based on a predefined ratio.\n",
    "\n",
    "We will also pass a random seed in order to make the split reproduceable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e1720a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_splits(numb_nodes, seed, train_ratio, val_ratio):\n",
    "    \"\"\"\n",
    "    TASK 5: Write a function which will return a numpy array of shape (numb_nodes,)\n",
    "    The array should contain strings (either \"train\", \"val\" or \"test\") such that\n",
    "    the proportion of total entries which are \"train\" is given by `train_ratio`,\n",
    "    the proportion of total entries which are \"val\" is given by `val_ratio`,\n",
    "    and the rest are \"test\"\n",
    "    \n",
    "    numb_nodes: int (number of nodes in graph)\n",
    "    seed: int (use this to set numpy random seed to make random splits reproduceable)\n",
    "    train_ratio: float (between 0 and 1)\n",
    "    val_ration: float (between 0 and 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    raise NotImplementedError\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b596f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(make_splits(20, 0, .7, .15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96f09b8",
   "metadata": {},
   "source": [
    "### __Useful functions__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcee6204",
   "metadata": {},
   "source": [
    "**Compute vacuity uncertainty**\n",
    "\n",
    "Write a function which computes vacuity uncertainty from Dirichlet parameters.\n",
    "\n",
    "Assume input is of shape (N, K), i.e. #nodes x #classes.\n",
    "\n",
    "Output should be of shape (N,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52abab62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vacuity(alphas):\n",
    "    \"\"\"\n",
    "    TASK 6: Write function to compute vacuity uncertainty for each node\n",
    "    \n",
    "    alphas: numpy array (2-dimensional) of shape (#nodes, #classes)\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcdd020",
   "metadata": {},
   "source": [
    "Simple check to see if our vacuity calculation works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1008cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_vacuity(np.array([[1,2,3],[1,1,1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37959a81",
   "metadata": {},
   "source": [
    "**Get belief mass vector from alpha vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261c7a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_belief(alphas):\n",
    "    return (alphas - 1) / np.sum(alphas, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b993ef3",
   "metadata": {},
   "source": [
    "Simple check to see if our belief mass calculation works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297b5cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_belief(np.array([[1,2,3],[1,1,1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a4ebed",
   "metadata": {},
   "source": [
    "**Compute dissonance uncertainty**\n",
    "\n",
    "A function which computes dissonance uncertainty from Dirichlet parameters.\n",
    "\n",
    "Assume input is of shape (N, K), i.e. #nodes x #classes.\n",
    "\n",
    "Output should be of shape (N,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5493b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dissonance_uncertainty(alphas):\n",
    "    alphas = np.array(alphas)  # will be used later when output might be a tf.Tensor\n",
    "    belief = get_belief(alphas)\n",
    "    dis_un = np.zeros(alphas.shape[0])\n",
    "\n",
    "    for i in range(alphas.shape[0]):  # for each node\n",
    "        b = belief[i]  # belief vector\n",
    "        numerator, denominator = np.abs(b[:, None] - b[None, :]), b[None, :] + b[:, None]\n",
    "        bal = 1 - np.true_divide(numerator, denominator, where=denominator != 0,\n",
    "                                 out=np.zeros_like(denominator)) - np.eye(len(b))\n",
    "        coefficients = b[:, None] * b[None, :] - np.diag(b ** 2)\n",
    "        denominator = np.sum(\n",
    "            b[None, :] * np.ones(belief.shape[1]) - np.diag(b), axis=-1, keepdims=True)\n",
    "        dis_un[i] = (coefficients * np.true_divide(bal, denominator, where=denominator != 0,\n",
    "                                                   out=np.zeros_like(bal))).sum()\n",
    "\n",
    "    return dis_un"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba5f7e5",
   "metadata": {},
   "source": [
    "Simple check to see if our belief mass calculation works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e579f72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dissonance_uncertainty(np.array([[20,20,20],[3,2,10]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc18392",
   "metadata": {},
   "source": [
    "**Get expected value of Dirichlet distribution from alpha vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e318563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_probability(alphas):\n",
    "    \"\"\"\n",
    "    TASK 7: Write function to compute expected probability for each node\n",
    "    \n",
    "    alphas: numpy array (2-dimensional) of shape (#nodes, #classes)\n",
    "    \n",
    "    return a numpy array (2-dimensional) of shape (#nodes, #classes)\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b8dd1f",
   "metadata": {},
   "source": [
    "Simple check to see if our vacuity calculation works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274a14b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_probability(np.array([[20, 20, 20],[1, 1, 1], [3, 2, 10]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcdfcf6",
   "metadata": {},
   "source": [
    "**Function to compute our graph: the connections between pixels/nodes found using k nearest neighbors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b64b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adj(x, k):\n",
    "    a = kneighbors_graph(x, k, include_self=False)\n",
    "    a = a + a.T  # to make graph symmetric (using k neighbours in \"either\" rather than \"mutual\" mode)\n",
    "    a[a > 1] = 1  # get rid of any edges we just made double\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0197676",
   "metadata": {},
   "source": [
    "### Prepare data for learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7023c0",
   "metadata": {},
   "source": [
    "We will use a package called `spektral` (information found [here](https://graphneural.network/)) which will do much of the heavy lifting for the graph-based deep learning.\n",
    "\n",
    "To use this library, we will first prepare our data into a `spektral` `Dataset` object in order to be able to make use of the library's functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1f1fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from spektral.data import Dataset, Graph\n",
    "from spektral.datasets.utils import DATASET_FOLDER\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    never_seen = None\n",
    "\n",
    "    def __init__(self,\n",
    "                 soil_moisture_data,\n",
    "                 labels,\n",
    "                 numb_neighbors,\n",
    "                 seed,\n",
    "                 region,\n",
    "                 splits,\n",
    "                 **kwargs):\n",
    "        self.x = soil_moisture_data  # inputs\n",
    "        self.y = labels  # ground truth (one-hot encoded)\n",
    "        self.numb_neighbs = numb_neighbors  # number of neighbors (for kNN) used to compute graph\n",
    "        self.mask_tr, self.mask_va, self.mask_te = None, None, None  # dataset splits\n",
    "        self.numb_op_classes = labels.shape[1]  # number of output classes (labels is one-hot encoded)\n",
    "        self.seed = seed  # random seed (can be used to compute average performance across different splits)\n",
    "        self.splits = splits  # train/validation/test splits\n",
    "        self.region = region  # used for folder naming purposes only\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    @property\n",
    "    def path(self):\n",
    "        # defines a folder in which dataset is saved\n",
    "        train_ratio = np.sum(self.splits == \"train\") / np.prod(self.splits.shape)\n",
    "        val_ratio = np.sum(self.splits == \"val\") / np.prod(self.splits.shape)\n",
    "        return os.path.join(\"spektral\",\n",
    "                            self.__class__.__name__\n",
    "                            ) + \"{}_classes{}_seed{}_train{:.3g}_val{:.3g}_neighbs_{}\".format(self.region,\n",
    "                                                                                      self.numb_op_classes,\n",
    "                                                                                      self.seed,\n",
    "                                                                                      train_ratio,\n",
    "                                                                                      val_ratio,\n",
    "                                                                                      self.numb_neighbs)\n",
    "\n",
    "    def download(self):\n",
    "        x = self.x\n",
    "        y = self.y\n",
    "        splits = self.splits\n",
    "\n",
    "        # get graph adjacency\n",
    "        a = get_adj(x, k=self.numb_neighbs)  # k is the number of nearest neighbours to look for\n",
    "\n",
    "        # Create the directory\n",
    "        if not os.path.isdir(\"spektral\"):\n",
    "            os.mkdir(\"spektral\")\n",
    "        os.mkdir(self.path)\n",
    "\n",
    "        filename = os.path.join(self.path, 'graph')\n",
    "        \n",
    "        np.savez(filename, x=x, a=a, y=y)\n",
    "\n",
    "    def read(self):\n",
    "        data = np.load(os.path.join(self.path, f'graph.npz'), allow_pickle=True)\n",
    "\n",
    "        x = data['x'].astype(np.float32)\n",
    "        a = data['a'].tolist()\n",
    "        y = data['y'].astype(np.uint8)\n",
    "\n",
    "        self.mask_tr = (self.splits == \"train\")\n",
    "        self.mask_va = (self.splits == \"val\")\n",
    "        self.mask_te = (self.splits == \"test\")\n",
    "\n",
    "        return [Graph(x=x, a=a, y=y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641195f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spektral.transforms import LayerPreprocess, AdjToSpTensor\n",
    "from spektral.layers import GCNConv\n",
    "from spektral.models import GCN\n",
    "\n",
    "random_seed = 0\n",
    "sm = CustomDataset(soil_moisture_data = model_inputs,\n",
    "                   labels = labels_oh,\n",
    "                   numb_neighbors = 5,\n",
    "                   seed = random_seed,\n",
    "                   region = region,\n",
    "                   splits = make_splits(labels_oh.shape[0], random_seed, train_ratio=.1, val_ratio=.05),\n",
    "                   transforms=[LayerPreprocess(GCNConv)]\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8de9257",
   "metadata": {},
   "source": [
    "### Perform learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea3d1d7",
   "metadata": {},
   "source": [
    "Create our Graph Convolutional Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368cc995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(random_seed)\n",
    "model = GCN(n_labels=labels_oh.shape[1], channels=32, output_activation=lambda z: tf.exp(z) + 1, l2_reg=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1e66cd",
   "metadata": {},
   "source": [
    "Our loss function (used to compile network before training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9a5a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquareErrorDirichlet(tf.keras.losses.Loss):\n",
    "    def __init__(self, name=\"square_error_dirichlet\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, y_true, alpha):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        strength = tf.reduce_sum(alpha, axis=1, keepdims=True)\n",
    "        prob = tf.divide(alpha, strength)\n",
    "        loss = tf.square(prob - y_true) + prob * (1 - prob) / (strength + 1.0)\n",
    "        return tf.reduce_sum(loss, axis=-1)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.01),\n",
    "    loss=SquareErrorDirichlet(),\n",
    "    weighted_metrics=[\"acc\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a3ad72",
   "metadata": {},
   "source": [
    "`weight_by_class` function assigns a weight to each node (e.g. masks out test nodes so that they do not contribute to the loss when training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f227a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_by_class(y, weights):\n",
    "    samples = y[weights != 0].sum(axis=0)  # count how many\n",
    "    samples = np.true_divide(len(weights), samples * len(samples[samples != 0]), out=0. * samples, where=samples != 0)\n",
    "    return (y * samples).max(axis=-1) * weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf702f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spektral.data import SingleLoader\n",
    "\n",
    "weights_tr, weights_va = [weight_by_class(sm[0].y, mask) for mask in [sm.mask_tr, sm.mask_va]]\n",
    "loader = SingleLoader(sm, sample_weights=weights_tr)\n",
    "steps_per_epoch = loader.steps_per_epoch\n",
    "loader_tr = loader.load()\n",
    "loader_va = SingleLoader(sm, sample_weights=weights_va).load()\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    loader_tr,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=loader_va,\n",
    "    validation_steps=steps_per_epoch,\n",
    "    epochs=2000,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=50,\n",
    "                                                restore_best_weights=True),\n",
    "               tf.keras.callbacks.ModelCheckpoint(os.path.join(\"GCN.h5\"),\n",
    "                                                  monitor=\"val_loss\", save_best_only=True,\n",
    "                                                  save_weights_only=True)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e556fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_all = SingleLoader(sm, epochs=1)\n",
    "inputs, outputs = loader_all.__next__()\n",
    "alpha = np.array(model(inputs, training=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe073504",
   "metadata": {},
   "source": [
    "`imageify` function used to restore the spatial dimension lost when flattening to remove unobserved regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2acfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imageify(values, shape, never_seen):\n",
    "    if values.ndim == 1:\n",
    "        image = np.zeros(shape) * np.nan\n",
    "    else:\n",
    "        image = np.zeros(shape + values.shape[1:]) * np.nan\n",
    "    image[~never_seen.reshape(shape)] = values\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38041ccc",
   "metadata": {},
   "source": [
    "Visualise our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb7d2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels_tr = imageify(alpha.argmax(axis=1), data.shape[:2], not_seen)\n",
    "predicted_labels_tr[imageify(sm.mask_tr, data.shape[:2], not_seen) != True] = np.nan\n",
    "plt.matshow(predicted_labels_tr, cmap=plt.cm.Accent, vmin=0, vmax=alpha.shape[1] - 1)\n",
    "plt.title(\"training nodes\")\n",
    "plt.show()\n",
    "\n",
    "predicted_labels_va = imageify(alpha.argmax(axis=1), data.shape[:2], not_seen)\n",
    "predicted_labels_va[imageify(sm.mask_va, data.shape[:2], not_seen) != True] = np.nan\n",
    "plt.matshow(predicted_labels_va, cmap=plt.cm.Accent, vmin=0, vmax=alpha.shape[1] - 1)\n",
    "plt.title(\"validation nodes\")\n",
    "plt.show()\n",
    "\n",
    "predicted_labels_te = imageify(alpha.argmax(axis=1), data.shape[:2], not_seen)\n",
    "predicted_labels_te[imageify(sm.mask_te, data.shape[:2], not_seen) != True] = np.nan\n",
    "plt.matshow(predicted_labels_te, cmap=plt.cm.Accent, vmin=0, vmax=alpha.shape[1] - 1)\n",
    "plt.title(\"predictions for test nodes\")\n",
    "plt.show()\n",
    "\n",
    "plt.matshow(imageify(sm[0].y.argmax(axis=1), data.shape[:2], not_seen), cmap=plt.cm.Accent, vmin=0, vmax=alpha.shape[1] - 1)\n",
    "plt.title(\"GT labels\")\n",
    "plt.show()\n",
    "\n",
    "right_vs_wrong = np.ones(alpha.shape[0:1] + (3,))\n",
    "right_vs_wrong[alpha.argmax(axis=1) == sm[0].y.argmax(axis=1)] = [0, 1, 0]\n",
    "right_vs_wrong[alpha.argmax(axis=1) != sm[0].y.argmax(axis=1)] = [1, 0, 0]\n",
    "plt.imshow(imageify(right_vs_wrong, data.shape[:2], not_seen))\n",
    "plt.title(\"Correct/Incorrect classification\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5e3a63",
   "metadata": {},
   "source": [
    "Visualize vacuity uncertainty (shown next to correct/incorrect classification to consider correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337b6b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2)\n",
    "cm = ax[0].matshow(imageify(get_vacuity(alpha), data.shape[:2], not_seen))\n",
    "# fig.colorbar(cm, ax=ax[0])\n",
    "ax[0].set_title(\"Vacuity uncertainty\")\n",
    "ax[1].imshow(imageify(right_vs_wrong, data.shape[:2], not_seen))\n",
    "ax[1].set_title(\"Correct/Incorrect classification\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e8f72b",
   "metadata": {},
   "source": [
    "Visualize dissonance uncertainty (shown next to correct/incorrect classification to consider correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e3d495",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2)\n",
    "cm = ax[0].matshow(imageify(dissonance_uncertainty(alpha), data.shape[:2], not_seen))\n",
    "# fig.colorbar(cm, ax=ax[0])\n",
    "ax[0].set_title(\"Dissonance uncertainty\")\n",
    "ax[1].imshow(imageify(right_vs_wrong, data.shape[:2], not_seen))\n",
    "ax[1].set_title(\"Correct/Incorrect classification\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0376a06e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
